name: Automl eval tables model
inputs:
- name: gcp_project_id
  type: String
- name: gcp_region
  type: String
- name: model_display_name
  type: String
- name: bucket_name
  type: String
- name: api_endpoint
  type: String
  optional: true
outputs:
- name: eval_data
  type: evals
- name: feat_list
  type: String
- name: MLPipeline UI metadata
  type: UI metadata
implementation:
  container:
    image: python:3.7
    command:
    - python3
    - -u
    - -c
    - "def _make_parent_dirs_and_return_path(file_path: str):\n    import os\n   \
      \ os.makedirs(os.path.dirname(file_path), exist_ok=True)\n    return file_path\n\
      \nclass OutputPath:\n    '''When creating component from function, OutputPath\
      \ should be used as function parameter annotation to tell the system that the\
      \ function wants to output data by writing it into a file with the given path\
      \ instead of returning the data from the function.'''\n    def __init__(self,\
      \ type=None):\n        self.type = type\n\nfrom typing import NamedTuple\n\n\
      def automl_eval_tables_model(\n\tgcp_project_id: str,\n\tgcp_region: str,\n\
      \  model_display_name: str,\n  bucket_name: str,\n  # gcs_path: str,\n  eval_data_path:\
      \ OutputPath('evals'),\n  api_endpoint: str = None,\n\n) -> NamedTuple('Outputs',\
      \ [\n    # ('evals_gcs_path', str),\n    ('feat_list', str)]):\n  import subprocess\n\
      \  import sys\n  subprocess.run([sys.executable, '-m', 'pip', 'install', 'googleapis-common-protos==1.6.0',\n\
      \     '--no-warn-script-location'], env={'PIP_DISABLE_PIP_VERSION_CHECK': '1'},\
      \ check=True)\n  subprocess.run([sys.executable, '-m', 'pip', 'install', 'google-cloud-automl==0.9.0',\n\
      \     '--no-warn-script-location'], env={'PIP_DISABLE_PIP_VERSION_CHECK': '1'},\
      \ check=True)\n  subprocess.run([sys.executable, '-m', 'pip', 'install',  #\
      \ 'google-cloud-storage',\n     'matplotlib', 'pathlib2', 'google-cloud-storage',\n\
      \     '--no-warn-script-location'], env={'PIP_DISABLE_PIP_VERSION_CHECK': '1'},\
      \ check=True)\n\n  import google\n  import json\n  import logging\n  import\
      \ pickle\n  import pathlib2\n\n  from google.api_core.client_options import\
      \ ClientOptions\n  from google.api_core import exceptions\n  from google.cloud\
      \ import automl_v1beta1 as automl\n  from google.cloud.automl_v1beta1 import\
      \ enums\n  from google.cloud import storage\n\n  def upload_blob(bucket_name,\
      \ source_file_name, destination_blob_name,\n      public_url=False):\n    \"\
      \"\"Uploads a file to the bucket.\"\"\"\n    # bucket_name = \"your-bucket-name\"\
      \n    # source_file_name = \"local/path/to/file\"\n    # destination_blob_name\
      \ = \"storage-object-name\"\n\n    storage_client = storage.Client()\n    bucket\
      \ = storage_client.bucket(bucket_name)\n    blob = bucket.blob(destination_blob_name)\n\
      \n    blob.upload_from_filename(source_file_name)\n\n    logging.info(\"File\
      \ {} uploaded to {}.\".format(\n            source_file_name, destination_blob_name))\n\
      \    if public_url:\n      blob.make_public()\n      logging.info(\"Blob {}\
      \ is publicly accessible at {}\".format(\n              blob.name, blob.public_url))\n\
      \    return blob.public_url\n\n  def get_model_details(client, model_display_name):\n\
      \    try:\n        model = client.get_model(model_display_name=model_display_name)\n\
      \    except exceptions.NotFound:\n        logging.info(\"Model %s not found.\"\
      \ % model_display_name)\n        return (None, None)\n\n    model = client.get_model(model_display_name=model_display_name)\n\
      \    # Retrieve deployment state.\n    if model.deployment_state == enums.Model.DeploymentState.DEPLOYED:\n\
      \        deployment_state = \"deployed\"\n    else:\n        deployment_state\
      \ = \"undeployed\"\n    # get features of top global importance\n    feat_list\
      \ = [\n        (column.feature_importance, column.column_display_name)\n   \
      \     for column in model.tables_model_metadata.tables_model_column_info\n \
      \   ]\n    feat_list.sort(reverse=True)\n    if len(feat_list) < 10:\n     \
      \   feat_to_show = len(feat_list)\n    else:\n        feat_to_show = 10\n\n\
      \    # Display the model information.\n    logging.info(\"Model name: {}\".format(model.name))\n\
      \    logging.info(\"Model id: {}\".format(model.name.split(\"/\")[-1]))\n  \
      \  logging.info(\"Model display name: {}\".format(model.display_name))\n   \
      \ logging.info(\"Features of top importance:\")\n    for feat in feat_list[:feat_to_show]:\n\
      \        logging.info(feat)\n    logging.info(\"Model create time:\")\n    logging.info(\"\
      \\tseconds: {}\".format(model.create_time.seconds))\n    logging.info(\"\\tnanos:\
      \ {}\".format(model.create_time.nanos))\n    logging.info(\"Model deployment\
      \ state: {}\".format(deployment_state))\n\n    generate_fi_ui(feat_list)\n\n\
      \    return (model, feat_list)\n\n  # TODO: generate ui-metadata for global\
      \ features importance.\n  # Try nbconvert-based viz?... though that doesn't\
      \ seem to be automatable?\n  def generate_fi_ui(feat_list):\n    import matplotlib.pyplot\
      \ as plt\n\n    image_suffix = 'arghh/testing/gfi.png'\n    res = list(zip(*feat_list))\n\
      \    x = list(res[0])\n    y = list(res[1])\n    y_pos = list(range(len(y)))\n\
      \    plt.barh(y_pos, x, alpha=0.5)\n    plt.yticks(y_pos, y)\n    plt.savefig('/gfi.png')\n\
      \    public_url = upload_blob(bucket_name, '/gfi.png', image_suffix, public_url=True)\n\
      \    logging.info('using image url {}'.format(public_url))\n\n    html_suffix\
      \ = 'arghh/testing/gfi.html'\n    with open('/gfi.html', 'w') as f:\n      #\
      \ f.write('<html><head></head><body><img src=\"https://storage.googleapis.com/aju-images/temp/gfi.png\"\
      \ /></body></html>')\n      f.write('<html><head></head><body><img src=\"{}\"\
      \ /></body></html>'.format(public_url))\n    upload_blob(bucket_name, '/gfi.html',\
      \ html_suffix)\n    html_source = 'gs://{}/{}'.format(bucket_name, html_suffix)\n\
      \    logging.info('metadata html source: {}'.format(html_source))\n\n    metadata\
      \ = {\n      'outputs' : [\n      {\n        'storage': 'inline',\n        'source':\
      \ '# Inline Markdown\\n[A link](https://www.kubeflow.org/)',\n        'type':\
      \ 'markdown',\n      },\n      {\n        'type': 'web-app',\n        'storage':\
      \ 'gcs',\n        # 'source': \"gs://aju-vtests2-misc/gfi.html\"\n        'source':\
      \ html_source\n      }\n      ]}\n    logging.info('using metadata dict {}'.format(json.dumps(metadata)))\n\
      \    with open('/mlpipeline-ui-metadata.json', 'w') as f:\n      json.dump(metadata,\
      \ f)\n\n  logging.getLogger().setLevel(logging.INFO)  # TODO: make level configurable\n\
      \  # TODO: we could instead check for region 'eu' and use 'eu-automl.googleapis.com:443'endpoint\n\
      \  # in that case, instead of requiring endpoint to be specified.\n  if api_endpoint:\n\
      \    client_options = ClientOptions(api_endpoint=api_endpoint)\n    client =\
      \ automl.TablesClient(project=gcp_project_id, region=gcp_region,\n        client_options=client_options)\n\
      \  else:\n    client = automl.TablesClient(project=gcp_project_id, region=gcp_region)\n\
      \n  (model, feat_list) = get_model_details(client, model_display_name)\n\n \
      \ evals = list(client.list_model_evaluations(model_display_name=model_display_name))\n\
      \  with open('temp_oput_regression', \"w\") as f:\n    f.write('Model evals:\\\
      n{}'.format(evals))\n  pstring = pickle.dumps(evals)\n  # pstring = pickled_eval.hex()\n\
      \  # copy_string_to_gcs(gcp_project_id, bucket_name, gcs_path, pstring)\n\n\
      \  # write to eval_data_path\n  if eval_data_path:\n    logging.info(\"eval_data_path:\
      \ %s\", eval_data_path)\n    try:\n      pathlib2.Path(eval_data_path).parent.mkdir(parents=True)\n\
      \    except FileExistsError:\n      pass\n    pathlib2.Path(eval_data_path).write_bytes(pstring)\n\
      \n  feat_list_string = json.dumps(feat_list)\n  # return(gcs_path, feat_list_string)\n\
      \  return(feat_list_string)\n\ndef _serialize_str(str_value: str) -> str:\n\
      \    if not isinstance(str_value, str):\n        raise TypeError('Value \"{}\"\
      \ has type \"{}\" instead of str.'.format(str(str_value), str(type(str_value))))\n\
      \    return str_value\n\nimport argparse\n_parser = argparse.ArgumentParser(prog='Automl\
      \ eval tables model', description='')\n_parser.add_argument(\"--gcp-project-id\"\
      , dest=\"gcp_project_id\", type=str, required=True, default=argparse.SUPPRESS)\n\
      _parser.add_argument(\"--gcp-region\", dest=\"gcp_region\", type=str, required=True,\
      \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--model-display-name\"\
      , dest=\"model_display_name\", type=str, required=True, default=argparse.SUPPRESS)\n\
      _parser.add_argument(\"--bucket-name\", dest=\"bucket_name\", type=str, required=True,\
      \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--api-endpoint\", dest=\"\
      api_endpoint\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
      --eval-data\", dest=\"eval_data_path\", type=_make_parent_dirs_and_return_path,\
      \ required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"----output-paths\"\
      , dest=\"_output_paths\", type=str, nargs=1)\n_parsed_args = vars(_parser.parse_args())\n\
      _output_files = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs = automl_eval_tables_model(**_parsed_args)\n\
      \nif not hasattr(_outputs, '__getitem__') or isinstance(_outputs, str):\n  \
      \  _outputs = [_outputs]\n\n_output_serializers = [\n    _serialize_str,\n\n\
      ]\n\nimport os\nfor idx, output_file in enumerate(_output_files):\n    try:\n\
      \        os.makedirs(os.path.dirname(output_file))\n    except OSError:\n  \
      \      pass\n    with open(output_file, 'w') as f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"
    args:
    - --gcp-project-id
    - inputValue: gcp_project_id
    - --gcp-region
    - inputValue: gcp_region
    - --model-display-name
    - inputValue: model_display_name
    - --bucket-name
    - inputValue: bucket_name
    - if:
        cond:
          isPresent: api_endpoint
        then:
        - --api-endpoint
        - inputValue: api_endpoint
    - --eval-data
    - outputPath: eval_data
    - '----output-paths'
    - outputPath: feat_list
    fileOutputs:
      MLPipeline UI metadata: /mlpipeline-ui-metadata.json
