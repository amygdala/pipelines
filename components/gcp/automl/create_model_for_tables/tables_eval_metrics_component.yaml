name: Automl eval metrics
inputs:
- name: gcp_project_id
  type: String
- name: gcp_region
  type: String
- name: model_display_name
  type: String
- name: bucket_name
  type: String
- name: eval_data
  type: evals
- name: api_endpoint
  type: String
  optional: true
- name: thresholds
  type: String
  default: '{"mean_absolute_error": 450}'
  optional: true
- name: confidence_threshold
  type: Float
  default: '0.5'
  optional: true
outputs:
- name: deploy
  type: Boolean
implementation:
  container:
    image: python:3.7
    command:
    - python3
    - -u
    - -c
    - "class InputPath:\n    '''When creating component from function, InputPath should\
      \ be used as function parameter annotation to tell the system to pass the *data\
      \ file path* to the function instead of passing the actual data.'''\n    def\
      \ __init__(self, type=None):\n        self.type = type\n\nfrom typing import\
      \ NamedTuple\n\ndef automl_eval_metrics(\n\tgcp_project_id: str,\n\tgcp_region:\
      \ str,\n  model_display_name: str,\n  bucket_name: str,\n  # gcs_path: str,\n\
      \  eval_data_path: InputPath('evals'),\n  api_endpoint: str = None,\n  # thresholds:\
      \ str = '{\"au_prc\": 0.9}',\n  thresholds: str = '{\"mean_absolute_error\"\
      : 450}',\n  confidence_threshold: float = 0.5  # for classification\n\n) ->\
      \ NamedTuple('Outputs', [('deploy', bool)]):\n  import subprocess\n  import\
      \ sys\n  subprocess.run([sys.executable, '-m', 'pip', 'install', 'googleapis-common-protos==1.6.0',\n\
      \      '--no-warn-script-location'], env={'PIP_DISABLE_PIP_VERSION_CHECK': '1'},\
      \ check=True)\n  subprocess.run([sys.executable, '-m', 'pip', 'install', 'google-cloud-automl==0.9.0',\n\
      \     '--no-warn-script-location'], env={'PIP_DISABLE_PIP_VERSION_CHECK': '1'},\
      \ check=True)\n\n  import google\n  import json\n  import logging\n  import\
      \ pickle\n  from google.api_core.client_options import ClientOptions\n  from\
      \ google.api_core import exceptions\n  from google.cloud import automl_v1beta1\
      \ as automl\n  from google.cloud.automl_v1beta1 import enums\n  # from google.cloud\
      \ import storage\n\n  # def get_string_from_gcs(project, bucket_name, gcs_path):\n\
      \  #   logging.info('Using bucket {} and path {}'.format(bucket_name, gcs_path))\n\
      \  #   storage_client = storage.Client(project=project)\n  #   bucket = storage_client.get_bucket(bucket_name)\n\
      \  #   blob = bucket.blob(gcs_path)\n  #   return blob.download_as_string()\n\
      \n  logging.getLogger().setLevel(logging.INFO)  # TODO: make level configurable\n\
      \  # TODO: we could instead check for region 'eu' and use 'eu-automl.googleapis.com:443'endpoint\n\
      \  # in that case, instead of requiring endpoint to be specified.\n  if api_endpoint:\n\
      \    client_options = ClientOptions(api_endpoint=api_endpoint)\n    client =\
      \ automl.TablesClient(project=gcp_project_id, region=gcp_region,\n        client_options=client_options)\n\
      \  else:\n    client = automl.TablesClient(project=gcp_project_id, region=gcp_region)\n\
      \n  thresholds_dict = json.loads(thresholds)\n  logging.info('thresholds dict:\
      \ {}'.format(thresholds_dict))\n\n  def regression_threshold_check(eval_info):\n\
      \    rmetrics = eval_info[1].regression_evaluation_metrics\n    logging.info('got\
      \ regression eval {}'.format(eval_info[1]))\n    eresults['root_mean_squared_error']\
      \ = rmetrics.root_mean_squared_error\n    eresults['mean_absolute_error'] =\
      \ rmetrics.mean_absolute_error\n    eresults['r_squared'] = rmetrics.r_squared\n\
      \    eresults['mean_absolute_percentage_error'] = rmetrics.mean_absolute_percentage_error\n\
      \    eresults['root_mean_squared_log_error'] = rmetrics.root_mean_squared_log_error\n\
      \    for k,v in thresholds_dict.items():\n      logging.info('k {}, v {}'.format(k,\
      \ v))\n      if k in ['root_mean_squared_error', 'mean_absolute_error', 'mean_absolute_percentage_error']:\n\
      \        if eresults[k] > v:\n          logging.info('{} > {}; returning False'.format(\n\
      \              eresults[k], v))\n          return False\n      elif eresults[k]\
      \ < v:\n        logging.info('{} < {}; returning False'.format(\n          \
      \  eresults[k], v))\n        return False\n    return True\n\n  def classif_threshold_check(eval_info):\n\
      \    example_count = eval_info[0].evaluated_example_count\n    print('Looking\
      \ for example_count {}'.format(example_count))\n    for e in eval_info[1:]:\
      \  # we know we don't want the first elt\n      if e.evaluated_example_count\
      \ == example_count:\n        # print('found relevant eval {}'.format(e))\n \
      \       eresults['au_prc'] = e.classification_evaluation_metrics.au_prc\n  \
      \      eresults['au_roc'] = e.classification_evaluation_metrics.au_roc\n   \
      \     eresults['log_loss'] = e.classification_evaluation_metrics.log_loss\n\
      \        for i in e.classification_evaluation_metrics.confidence_metrics_entry:\n\
      \          if i.confidence_threshold >= confidence_threshold:\n            eresults['recall']\
      \ = i.recall\n            eresults['precision'] = i.precision\n            eresults['f1_score']\
      \ = i.f1_score\n            break\n        break\n    logging.info('eresults:\
      \ {}'.format(eresults))\n    for k,v in thresholds_dict.items():\n      logging.info('k\
      \ {}, v {}'.format(k, v))\n      if k == 'log_loss':\n        if eresults[k]\
      \ > v:\n          logging.info('{} > {}; returning False'.format(\n        \
      \      eresults[k], v))\n          return False\n      else:\n        if eresults[k]\
      \ < v:\n          logging.info('{} < {}; returning False'.format(\n        \
      \      eresults[k], v))\n          return False\n    return True\n\n  def generate_cm_metadata():\n\
      \    pass\n\n  # testing...\n  with open(eval_data_path, 'rb') as f:\n    logging.info('successfully\
      \ opened eval_data_path {}'.format(eval_data_path))\n    try:\n      eval_info\
      \ = pickle.loads(f.read())\n      eresults = {}\n      # TODO: add handling\
      \ of confusion matrix stuff for binary classif case..\n      # eval_string =\
      \ get_string_from_gcs(gcp_project_id, bucket_name, gcs_path)\n      # eval_info\
      \ = pickle.loads(eval_string)\n\n      classif = False\n      binary_classif\
      \ = False\n      regression = False\n      # TODO: ughh... what's the right\
      \ way to figure out the model type?\n      if eval_info[1].regression_evaluation_metrics\
      \ and eval_info[1].regression_evaluation_metrics.root_mean_squared_error:\n\
      \        regression=True\n        logging.info('found regression metrics {}'.format(eval_info[1].regression_evaluation_metrics))\n\
      \      if eval_info[1].classification_evaluation_metrics and eval_info[1].classification_evaluation_metrics.au_prc:\n\
      \        classif = True\n        logging.info('found classification metrics\
      \ {}'.format(eval_info[1].classification_evaluation_metrics))\n        # TODO:\
      \ detect binary classification case\n\n      if regression and thresholds_dict:\n\
      \        res = regression_threshold_check(eval_info)\n        logging.info('deploy\
      \ flag: {}'.format(res))\n        # TODO: generate ui-metadata as appropriate\n\
      \        return res\n      elif classif and thresholds_dict:\n        res =\
      \ classif_threshold_check(eval_info)\n        logging.info('deploy flag: {}'.format(res))\n\
      \        # TODO: generate confusion matrix ui-metadata as approp etc.\n    \
      \    if binary_classif:\n          generate_cm_metadata()\n        return res\n\
      \      else:\n        return True\n    except Exception as e:\n      logging.warning(e)\n\
      \      # If can't reconstruct the eval, or don't have thresholds defined,\n\
      \      # return True as a signal to deploy.\n      # TODO: is this the right\
      \ default?\n      return True\n\ndef _serialize_bool(bool_value: bool) -> str:\n\
      \    if isinstance(bool_value, str):\n        return bool_value\n    if not\
      \ isinstance(bool_value, bool):\n        raise TypeError('Value \"{}\" has type\
      \ \"{}\" instead of bool.'.format(str(bool_value), str(type(bool_value))))\n\
      \    return str(bool_value)\n\nimport argparse\n_parser = argparse.ArgumentParser(prog='Automl\
      \ eval metrics', description='')\n_parser.add_argument(\"--gcp-project-id\"\
      , dest=\"gcp_project_id\", type=str, required=True, default=argparse.SUPPRESS)\n\
      _parser.add_argument(\"--gcp-region\", dest=\"gcp_region\", type=str, required=True,\
      \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--model-display-name\"\
      , dest=\"model_display_name\", type=str, required=True, default=argparse.SUPPRESS)\n\
      _parser.add_argument(\"--bucket-name\", dest=\"bucket_name\", type=str, required=True,\
      \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--eval-data\", dest=\"\
      eval_data_path\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
      --api-endpoint\", dest=\"api_endpoint\", type=str, required=False, default=argparse.SUPPRESS)\n\
      _parser.add_argument(\"--thresholds\", dest=\"thresholds\", type=str, required=False,\
      \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--confidence-threshold\"\
      , dest=\"confidence_threshold\", type=float, required=False, default=argparse.SUPPRESS)\n\
      _parser.add_argument(\"----output-paths\", dest=\"_output_paths\", type=str,\
      \ nargs=1)\n_parsed_args = vars(_parser.parse_args())\n_output_files = _parsed_args.pop(\"\
      _output_paths\", [])\n\n_outputs = automl_eval_metrics(**_parsed_args)\n\nif\
      \ not hasattr(_outputs, '__getitem__') or isinstance(_outputs, str):\n    _outputs\
      \ = [_outputs]\n\n_output_serializers = [\n    _serialize_bool\n]\n\nimport\
      \ os\nfor idx, output_file in enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n\
      \    except OSError:\n        pass\n    with open(output_file, 'w') as f:\n\
      \        f.write(_output_serializers[idx](_outputs[idx]))\n"
    args:
    - --gcp-project-id
    - inputValue: gcp_project_id
    - --gcp-region
    - inputValue: gcp_region
    - --model-display-name
    - inputValue: model_display_name
    - --bucket-name
    - inputValue: bucket_name
    - --eval-data
    - inputPath: eval_data
    - if:
        cond:
          isPresent: api_endpoint
        then:
        - --api-endpoint
        - inputValue: api_endpoint
    - if:
        cond:
          isPresent: thresholds
        then:
        - --thresholds
        - inputValue: thresholds
    - if:
        cond:
          isPresent: confidence_threshold
        then:
        - --confidence-threshold
        - inputValue: confidence_threshold
    - '----output-paths'
    - outputPath: deploy
